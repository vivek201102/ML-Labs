{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1WlTM1ZANldUyDb84lsPuKsWNMYerD3sk",
      "authorship_tag": "ABX9TyM0g2RPMpLPK7fhOw2SmNM/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivek201102/ML-Labs/blob/master/Lab_07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekdmhn-bEZep",
        "outputId": "1c937c49-e3f1-4b42-b936-b22119e0c2dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import math\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "oPIqnvcREfMH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Datasets/BuyComputer.csv\")\n",
        "df.drop(columns=['User ID'], axis=1, inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "YgCU0Tc2EhcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = np.array(df.iloc[:, :-1], dtype='float32')\n",
        "y_data = np.array(df.iloc[:, -1], dtype='float32')\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_data,\n",
        "y_data, test_size = 0.25, random_state = 4)\n",
        "\n",
        "\n",
        "sc = StandardScaler()\n",
        "x_train = sc.fit_transform(x_train)\n",
        "x_test = sc.fit_transform(x_test)\n",
        "\n",
        "\n",
        "inputs = torch.from_numpy(x_train)\n",
        "targets = torch.from_numpy(y_train)\n",
        "\n",
        "dataset = TensorDataset(inputs, targets)\n"
      ],
      "metadata": {
        "id": "ELNSjT2eNP_o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = torch.randn(2,1, requires_grad=True)\n",
        "b = torch.randn(1,1, requires_grad=True)\n",
        "\n",
        "\n",
        "def model(x):\n",
        "  temp = w.T @ x + b\n",
        "  expo = math.exp(-1 * temp)\n",
        "  ans = 1 / (1 + expo)\n",
        "  return ans\n",
        "\n",
        "\n",
        "def bce_loss_function(prediction, target):\n",
        "  a = math.log(prediction)\n",
        "  b= math.log(1 - prediction)\n",
        "  loss = (-target * a) - ((1 - target) * b)\n",
        "  return torch.sum(loss)\n",
        "\n",
        "\n",
        "print(\"weights: \", w)\n",
        "print(\"bias: \", b)\n",
        "\n",
        "for x,y in dataset:\n",
        "  prediction = model(x)\n",
        "  loss = bce_loss_function(prediction, y)\n",
        "\n",
        "print(\"Inital Loss: \", loss)\n",
        "\n",
        "epochs = 100\n",
        "for i in range(epochs):\n",
        "  for x,y in dataset:\n",
        "    pred = model(x)\n",
        "    loss_val = bce_loss_function(pred, y)\n",
        "\n",
        "    loss_val.backward()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      w -= w.grad * 1e-5\n",
        "      b -= b.grad * 1e-5\n",
        "\n",
        "      w.grad.zero_()\n",
        "      b.grad.zero_()\n",
        "\n",
        "\n",
        "for x,y in dataset:\n",
        "  prediction = model(x)\n",
        "  loss = bce_loss_function(prediction, y)\n",
        "\n",
        "print(\"Final Loss: \", loss)\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"weights: \", w)\n",
        "print(\"bias: \", b)"
      ],
      "metadata": {
        "id": "j8cnfbReRxM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Datasets/BuyComputer.csv\")\n",
        "data.drop(columns=['User ID',],axis=1,inplace=True)\n",
        "\n",
        "\n",
        "#Declaring X as all columns excluding last\n",
        "X = data.iloc[:,:-1].values\n",
        "\n",
        "#Declare label as last column in the source file\n",
        "Y = data.iloc[:,-1].values\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, random_state=4)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "y_pred = []\n",
        "len_x = len(X_train[0])\n",
        "w = []\n",
        "b = 0.2\n",
        "print(len_x)\n",
        "entries = len(X_train[:,0])\n",
        "\n",
        "for weights in range(len_x):\n",
        "    w.append(0)\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1/(1 + np.exp(-z))\n",
        "\n",
        "#Prediction\n",
        "def predict(input):\n",
        "    z = np.dot(input, w) + b\n",
        "    h= sigmoid(z)\n",
        "    for i in range(len(h)):\n",
        "        if(h[i]>=0.5):\n",
        "            h[i]=1\n",
        "        else:\n",
        "            h[i]=0\n",
        "    return h\n",
        "    #Loss function\n",
        "def loss_func(y, y1):\n",
        "    total_bce_loss = np.sum(-y * np.log(y1) - (1 - y) * np.log(1 - y1))\n",
        "    m = y.shape[0]\n",
        "    j = total_bce_loss /m\n",
        "    return j\n",
        "\n",
        "dw = []\n",
        "db = 0\n",
        "J = 0\n",
        "alpha = 0.1\n",
        "for x in range(len_x):\n",
        "    dw.append(0)\n",
        "\n",
        "for i in range(1000):\n",
        "    z = np.dot(X_train, w) + b\n",
        "    y_pred = sigmoid(z)\n",
        "    l = loss_func(y_pred, y_train)\n",
        "    dw = np.dot((y_pred-y_train).T, X_train)/X_train.shape[0]\n",
        "    db = np.mean(y_pred-y_train)\n",
        "    w = w - alpha * dw\n",
        "    b = b - alpha* db\n",
        "\n",
        "\n",
        "y_pred=predict(X_train)\n",
        "print(y_pred)\n",
        "\n",
        "\n",
        "\n",
        "acc = np.sum([y_test[i] == (y_pred[i]) for i in range(len(y_test))])/len(y_test)\n",
        "print(\"Accuracy:\",acc*100,\"%\")\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logisticRegr = LogisticRegression()\n",
        "logisticRegr.fit(X_train, y_train)\n",
        "# a = logisticRegr.predict(X_test[0].reshape(1,-1))\n",
        "# a\n",
        "predictions = logisticRegr.predict(X_test)\n",
        "print(predictions)\n",
        "\n",
        "\n",
        "score = logisticRegr.score(X_test, y_test)\n",
        "print(score)"
      ],
      "metadata": {
        "id": "H6bi4D_3P9JZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "import math\n",
        "# reading the csv file, del 2 columns from the file, checking first few rows of the file\n",
        "from google.colab import drive\n",
        "\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Datasets/BuyComputer.csv\")\n",
        "dataset.drop(columns=['User ID',],axis=1,inplace=True)\n",
        "label = dataset.iloc[:,-1].values\n",
        "X = dataset.drop(\"Purchased\" ,axis= 1)\n",
        "# Splitting data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_Train, X_Test, y_Train, y_Test = train_test_split(X,\n",
        "label, test_size = 0.30, random_state = 91)\n",
        "# Sacaling data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_Train = sc.fit_transform(X_Train)\n",
        "X_Test = sc.transform(X_Test)\n",
        "\n",
        "# print(X_Train)\n",
        "X_Train=torch.from_numpy(X_Train)\n",
        "y_Train=torch.from_numpy(y_Train)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logisticRegr = LogisticRegression(random_state = 4)\n",
        "logisticRegr.fit(X_Train, y_Train)\n",
        "predictions = logisticRegr.predict(X_Test)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "HPbp3caDWp-4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}